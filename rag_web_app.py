import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from PIL import Image
import tempfile

# ---- Configuration ----
st.set_page_config(page_title="Gemini PDF Chat", layout="wide", page_icon="üìÑ")

# ---- Sidebar (Logo + Chat History) ----
with st.sidebar:
    try:
        logo = Image.open("logo.png")
        st.image(logo, width=150)
    except:
        st.markdown("üß† **Gemini PDF Chat**")
    st.markdown("---")
    st.markdown("### üìö Previous Questions")
    if "chat_history" in st.session_state:
        for idx, (q, _) in enumerate(st.session_state.chat_history):
            st.markdown(f"**{idx+1}.** {q[:50]}...")

# ---- Gemini API Key (from Streamlit secrets or ENV) ----
# Set your key in `.streamlit/secrets.toml` as GEMINI_API_KEY or set the
# environment variable GEMINI_API_KEY. Do NOT commit your real key.
api_key = None
if hasattr(st, "secrets") and "GEMINI_API_KEY" in st.secrets:
    api_key = st.secrets["GEMINI_API_KEY"]
else:
    api_key = os.getenv("GEMINI_API_KEY")

if not api_key:
    st.warning(
        "GEMINI_API_KEY not found. Set it in .streamlit/secrets.toml (GEMINI_API_KEY) or as the environment variable GEMINI_API_KEY."
    )
    st.stop()

# ---- Session State Init ----
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# ---- Title & Upload ----
st.title("RAGified Reader")
uploaded_file = st.file_uploader("üì§ Upload a PDF file", type="pdf")

# ---- PDF Processing ----
if uploaded_file and st.session_state.qa_chain is None:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    st.info("üîç Extracting content and indexing...")
    loader = PyPDFLoader(tmp_path)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_documents(docs)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    vectordb = Chroma.from_documents(chunks, embedding=embeddings)
    retriever = vectordb.as_retriever(search_kwargs={"k": 10})

    model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key=api_key)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=model,
        retriever=retriever,
        return_source_documents=False
    )
    st.session_state.qa_chain = qa_chain
    st.success("‚úÖ PDF indexed! Start chatting below.")

# ---- Chat Interface ----
if st.session_state.qa_chain:
    user_input = st.chat_input("üí¨ Ask your question here...")
    if user_input:
        with st.spinner("ü§ñ Generating response..."):
            result = st.session_state.qa_chain({
                "question": user_input,
                "chat_history": st.session_state.chat_history
            })
            st.session_state.chat_history.append((user_input, result["answer"]))

    # Display Chat History (ChatGPT Style)
    for q, a in st.session_state.chat_history:
        with st.chat_message("user", avatar="üë§"):
            st.markdown(q)
        with st.chat_message("assistant", avatar="ü§ñ"):
            st.markdown(a)
